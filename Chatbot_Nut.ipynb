{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Chatbot_Nut.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDkdu0Whxkfl"
      },
      "source": [
        "# Chatbot v0.1 Según enfermedad o IMC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7xxp19zBhD4",
        "outputId": "25d73ba8-16df-4fa4-8c1d-8051f9c70a0f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTaHZ1Few0A8",
        "outputId": "c406ca62-ba6b-4be8-e2df-e8ded56cce5a"
      },
      "source": [
        "import nltk #herramientas de lenguaje natural\n",
        "import numpy as np # Para la estructuras de datos y matrices)\n",
        "import random\n",
        "import string #usar y procesar cadenas de texto\n",
        "#nuevas librerias---\n",
        "import torch #libreria para el aprendizaje automatico, utiliza los tensorflow, ejecuta el codigo de forma nativa usando la GPU\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mabUOMWp5xOe",
        "outputId": "e7b214ab-e58b-4b87-f92e-8e7a3fceacf9"
      },
      "source": [
        "#base de datos de dieta\n",
        "f=open('/content/basededatosNut.json','r',errors = 'ignore')\n",
        "\n",
        "raw=f.read()\n",
        "raw=raw.lower()# convertir a minusculas\n",
        "\n",
        "sent_tokens = nltk.sent_tokenize(raw)# convierte a lista de oraciones \n",
        "word_tokens = nltk.word_tokenize(raw)# convierte a una lista de palabras\n",
        "\n",
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "#WordNet es un diccionario de inglés de orientación semántica incluido en NLTK.\n",
        "\n",
        "\n",
        "#lematizar palabras\n",
        "def LemTokens(tokens):\n",
        "    return [lemmer.lemmatize(token) for token in tokens] \n",
        "\n",
        "\n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation) #Puntuacion de palabras \n",
        "\n",
        "#print(remove_punct_dict)\n",
        "\n",
        "\n",
        "def LemNormalize(text):\n",
        "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
        "\n",
        "\n",
        "\n",
        "#ENTRADAS Y RESPUESTAS PREDETERMINADAS\n",
        "GREETING_INPUTS = (\"Hola\")\n",
        "GREETING_RESPONSES = [ \"Bienvenido\", \"Hola\"]\n",
        "\n",
        "#funcion para una respuesta predeterminada \n",
        "def greeting(sentence):\n",
        " \n",
        "    for word in sentence.split():\n",
        "        if word.lower() in GREETING_INPUTS:\n",
        "            return random.choice(GREETING_RESPONSES)\n",
        "        \n",
        "#SKLEARN biblioteca para aprendizaje automático\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "#Matrices de palabras similares\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "#funcion de procesamiento\n",
        "def response(user_response):\n",
        "    robo_response=''\n",
        "    sent_tokens.append(user_response)\n",
        "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
        "    tfidf = TfidfVec.fit_transform(sent_tokens) #vectoriza la palabras\n",
        "    vals = cosine_similarity(tfidf[-1], tfidf) #Calcule la similitud de coseno entre muestras en X e Y.\n",
        "    idx=vals.argsort()[0][-2] #compara las similitudes en mun margen de dos items\n",
        "    flat = vals.flatten() #Devuelve una copia de la matriz contraída en una dimensión.\n",
        "    flat.sort()\n",
        "    req_tfidf = flat[-2]\n",
        "    if(req_tfidf==0):\n",
        "        robo_response=robo_response+\"Disculpa, no te comprendo\"\n",
        "        return robo_response\n",
        "    else:\n",
        "        robo_response = robo_response+sent_tokens[idx]\n",
        "        return robo_response\n",
        "\n",
        "flag=True\n",
        "\n",
        "\n",
        "\n",
        "print(\"BotNut: Mi nombre es BotNut. Responderé a tus consultas sobre dietas. Si desea salir, escriba ¡Adiós!\")\n",
        "while(flag==True):\n",
        "    user_response = input()\n",
        "    user_response=user_response.lower()\n",
        "    if(user_response!='adios'):\n",
        "        if(user_response=='gracias' or user_response=='gracias a ti' ):\n",
        "            flag=False\n",
        "            print(\"BotNut: de nada..\")\n",
        "        else:\n",
        "            if(greeting(user_response)!=None):\n",
        "                print(\"BotNut: \"+greeting(user_response))\n",
        "            else:\n",
        "                print(\"BotNut: \",end=\"\")\n",
        "                print(response(user_response))\n",
        "                sent_tokens.remove(user_response)\n",
        "    else:\n",
        "        flag=False\n",
        "        print(\"BotNut: ¡Adiós! Cuídate..\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BotNut: Mi nombre es BotNut. Responderé a tus consultas sobre dietas. Si desea salir, escriba ¡Adiós!\n",
            "saludo\n",
            "BotNut: "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"converzacion\": [\n",
            "    {\n",
            "      \"etiqueta\": \"saludo\",\n",
            "      \"patrones\": [\n",
            "        \"hola\",\n",
            "        \"oye\",\n",
            "        \"cómo estás\",\n",
            "        \"¿hay alguien ahí?\n",
            "gracias\n",
            "BotNut: de nada..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klHTLvk4N2Uk"
      },
      "source": [
        "#Chatbot v2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOnC7rQuN5Ul"
      },
      "source": [
        "import nltk #herramientas de lenguaje natural\n",
        "import numpy as np #Para la estructuras de datos y matrices\n",
        "import random #herramienta para generar números pseudoaleatorios\n",
        "import json #herramienta para el intercambio de datos y para analizar los datos\n",
        "\n",
        "import torch #libreria para el aprendizaje automatico, utiliza los tensorflow, ejecuta el codigo de forma nativa usando la GPU\n",
        "import torch.nn as nn #Permite construir redes neuronales\n",
        "from torch.utils.data import Dataset, DataLoader #Permite indicar el conjunto de datos que que se cargará\n",
        "\n",
        "from nltk_utils import bag_of_words, tokenize, stem\n",
        "from modelo import NeuralNet #contiene las funciones de torch redes neuronales\n",
        "\n",
        "nltk.download('punkt') #Este tokenizer se entrenó bien para trabajar con muchos idiomas.\n",
        "\n",
        "#cargamos nuestra bd\n",
        "with open('basededatos.json', 'r') as f:\n",
        "    intenciones = json.load(f)\n",
        "\n",
        "todas_las_palabras = []\n",
        "etiquetas = []\n",
        "xy = []\n",
        "# recorrer cada oración en nuestros patrones de intenciones\n",
        "for inten in intenciones['intenciones']:\n",
        "    etiqueta = inten['etiqueta']\n",
        "    # agregar a la lista de etiquetas\n",
        "    etiquetas.append(etiqueta)\n",
        "    for patron in inten['patrones']:\n",
        "        # tokenizar cada palabra de la oración\n",
        "        w = tokenize(patron)\n",
        "        # agregar a nuestra lista de palabras\n",
        "        todas_las_palabras.extend(w)\n",
        "        # agregar al par xy\n",
        "        xy.append((w, etiqueta))\n",
        "\n",
        "# ignorar palabra\n",
        "ignorar_palabras = ['?', '.', '!']\n",
        "todas_las_palabras = [stem(w) for w in todas_las_palabras if w not in ignorar_palabras]\n",
        "# eliminar duplicados y ordenar\n",
        "todas_las_palabras = sorted(set(todas_las_palabras))\n",
        "etiquetas = sorted(set(etiquetas))\n",
        "\n",
        "print(len(xy), \"patrones\")\n",
        "print(len(etiquetas), \"etiquetas:\", etiquetas)\n",
        "print(len(todas_las_palabras), \"palabras derivadas unicas:\", todas_las_palabras)\n",
        "\n",
        "# crear datos de entrenamiento\n",
        "X_cola = []\n",
        "Y_cola = []\n",
        "for (patron_de_oracion, etiqueta) in xy:\n",
        "    # X: bolsa de palabras para cada patron de sentencia\n",
        "    bolsa = bag_of_words(patron_de_oracion, todas_las_palabras)\n",
        "    X_cola.append(bolsa)\n",
        "    # y: PyTorch CrossEntropyLoss solo necesita etiquetas de clase, no one-hot\n",
        "    ETIQUETA_ = etiquetas.index(etiqueta)\n",
        "    Y_cola.append(ETIQUETA_)\n",
        "\n",
        "X_cola = np.array(X_cola)\n",
        "Y_cola = np.array(Y_cola)\n",
        "\n",
        "# Hiperparámetros\n",
        "interaciones = 1000 #NUMERO DE INTERACIONES\n",
        "batch_size = 8      #tamanio de lote\n",
        "tasa_de_aprendizaje = 0.001\n",
        "tamanio_de_entrada = len(X_cola[0])\n",
        "tamanio_oculto = 8\n",
        "tamanio_de_salida = len(etiquetas)\n",
        "print(tamanio_de_entrada, tamanio_de_salida)\n",
        "\n",
        "class ChatDataset(Dataset):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.n_samples = len(X_cola)\n",
        "        self.x_data = X_cola\n",
        "        self.y_data = Y_cola\n",
        "\n",
        "    # Admite la indexación de modo que el conjunto de datos [i] se pueda utilizar para obtener la i-ésima muestra\n",
        "    def __getitem__(self, index):\n",
        "        return self.x_data[index], self.y_data[index]\n",
        "\n",
        "    # podemos llamar a len (conjunto de datos) para devolver el tamaño\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "\n",
        "dataset = ChatDataset()\n",
        "cargador_de_cola = DataLoader(dataset=dataset,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=True,\n",
        "                          num_workers=0)\n",
        "\n",
        "dispositivo = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "modelo = NeuralNet(tamanio_de_entrada, tamanio_oculto, tamanio_de_salida).to(dispositivo)\n",
        "\n",
        "# Pérdida y optimizacion\n",
        "criterio = nn.CrossEntropyLoss()\n",
        "optimizador = torch.optim.Adam(modelo.parameters(), lr=tasa_de_aprendizaje)\n",
        "\n",
        "# Entrena el modelo\n",
        "for nivel in range(interaciones):\n",
        "    for (PALABRAS, ETIQUETAS) in cargador_de_cola:\n",
        "        PALABRAS = PALABRAS.to(dispositivo)\n",
        "        ETIQUETAS = ETIQUETAS.to(dtype=torch.long).to(dispositivo)\n",
        "        \n",
        "        # Pase adelantado\n",
        "        SALIDAS = modelo(PALABRAS)\n",
        "        # si y sería one-hot, debemos aplicar\n",
        "        # etiquetas = antorcha.max (etiquetas, 1) [1]\n",
        "        PERDIDAS = criterio(SALIDAS, ETIQUETAS)\n",
        "        \n",
        "        # Retroceder y optimizar\n",
        "        optimizador.zero_grad()\n",
        "        PERDIDAS.backward()\n",
        "        optimizador.step()\n",
        "        \n",
        "    if (nivel+1) % 100 == 0:\n",
        "        print (f'interaciones [{nivel+1}/{interaciones}], perdidos: {PERDIDAS.item():.4f}')\n",
        "\n",
        "\n",
        "print(f'perdida final: {PERDIDAS.item():.4f}')\n",
        "\n",
        "datos = {\n",
        "\"estado_del_modelo\": modelo.state_dict(),\n",
        "\"tamanio_de_entrada\": tamanio_de_entrada,\n",
        "\"tamanio_oculto\": tamanio_oculto,\n",
        "\"tamanio_de_salida\": tamanio_de_salida,\n",
        "\"todas_las_palabras\": todas_las_palabras,\n",
        "\"etiquetas\": etiquetas\n",
        "}\n",
        "\n",
        "FILE = \"datos.pth\"\n",
        "torch.save(datos, FILE)\n",
        "\n",
        "print(f'Entrenamiento completado, archivo guardado en: {FILE}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fJlKEjzN-z0"
      },
      "source": [
        "import random\n",
        "import json\n",
        "\n",
        "import torch #libreria para el aprendizaje automatico, utiliza los tensorflow, ejecuta el codigo de forma nativa usando la GPU\n",
        "\n",
        "from modelo import NeuralNet\n",
        "from nltk_utils import bag_of_words, tokenize\n",
        "\n",
        "#declaramos el objeto torch\n",
        "dispositivo = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#cargamos nuestra bd\n",
        "with open('basededatos.json', 'r') as json_data:\n",
        "    INTENCIONES = json.load(json_data)\n",
        "\n",
        "#cargamos losdatos de entrenamiento\n",
        "FILE = \"datos.pth\"\n",
        "datos = torch.load(FILE)\n",
        "\n",
        "#obtenemos los paraetros\n",
        "_tamanio_entrada = datos[\"tamanio_de_entrada\"]\n",
        "_tamanio_oculto = datos[\"tamanio_oculto\"]\n",
        "_tamanio_salida = datos[\"tamanio_de_salida\"]\n",
        "_todas_las_palabras = datos['todas_las_palabras']\n",
        "_etiqueta = datos['etiquetas']\n",
        "_estado_modelo = datos[\"estado_del_modelo\"]\n",
        "\n",
        "#crea el modelo de reconocimiento\n",
        "MODELO = NeuralNet(_tamanio_entrada, _tamanio_oculto, _tamanio_salida).to(dispositivo)\n",
        "MODELO.load_state_dict(_estado_modelo)\n",
        "MODELO.eval() #evalua las respuestas\n",
        "\n",
        "\n",
        "bot_name = \"PsicoRobo\"\n",
        "print(f\"{bot_name}: Mi nombre es PsicoRobo. Responderé a tus consultas, si desea salir, escriba adios\")\n",
        "while True:\n",
        "    sentencias = input(\"Tu: \")\n",
        "    if sentencias == \"adios\":\n",
        "        print(f\"{bot_name}:Que tengas un buen día\")\n",
        "        break\n",
        "\n",
        "    sentencias = tokenize(sentencias) #tokenizamos las palabras\n",
        "    X = bag_of_words(sentencias, _todas_las_palabras) #coincidencia de palabras con la bd json\n",
        "    X = X.reshape(1, X.shape[0]) \n",
        "    X = torch.from_numpy(X).to(dispositivo) #Objeto de entrenamiento y posibles respuestas\n",
        "\n",
        "    salida = MODELO(X) \n",
        "    _, prediccion = torch.max(salida, dim=1)\n",
        "\n",
        "    ETIQUETA = _etiqueta[prediccion.item()] #etiquetas y predicciones\n",
        "\n",
        "    problemas = torch.softmax(salida, dim=1) #entrada de n dimensiones y rescalado de salidas de n dimensiones\n",
        "    probabilidad = problemas[0][prediccion.item()] #prediccion de la respuesta mas acertada\n",
        "    if probabilidad.item() > 0.75: #probabilidades mayores a 75%\n",
        "        for intenc in INTENCIONES['intenciones']: #posibles intenciones\n",
        "            if ETIQUETA == intenc[\"etiqueta\"]:\n",
        "                print(f\"{bot_name}: {random.choice(intenc['respuestas'])}\")\n",
        "    else:\n",
        "        print(f\"{bot_name}: No entiendo...\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}